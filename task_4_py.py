# -*- coding: utf-8 -*-
"""task 4.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K0KznSBp7_DirzKb9piLzkWp1uv2ojDM
"""

pip install langchain langchain-openai tavily-python langchain-community

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from tavily import TavilyClient
import os

# Add API keys
os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_KEY"
os.environ["TAVILY_API_KEY"] = "YOUR_TAVILY_KEY"

llm = ChatOpenAI(model="gpt-4o-mini")
tavily = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])


# Step 1: Generate Sub-Questions
def generate_sub_questions(topic):
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You generate exactly 3 sub-questions."),
        ("user", f"Generate 3 sub-questions on: {topic}")
    ])

    msg = llm.invoke(prompt.format())
    lines = msg.content.split("\n")
    cleaned = [line[line.find('.')+1:].strip() for line in lines]

    return cleaned


# Step 2: Use Tavily search for each question
def search_answer(question):
    result = tavily.search(query=question, max_results=3)

    # Extract short text snippets
    snippets = [item["content"] for item in result["results"]]
    text = "\n".join(snippets[:3])

    # Summarize using LLM
    prompt = ChatPromptTemplate.from_template(
        "Summarize the following in 3 lines:\n{text}"
    )
    summary = llm.invoke(prompt.format(text=text))

    return summary.content


# Step 3: Summarize all answers
def summarize(topic, answers):
    text = "\n".join(answers)
    prompt = ChatPromptTemplate.from_template("""
    Topic: {topic}
    Combine the following notes into one clean paragraph:
    {text}
    """)

    return llm.invoke(prompt.format(topic=topic, text=text)).content


# ---------- MAIN PIPELINE ----------
def main():
    topic = input("Enter research topic: ")

    # Step 1
    sub_qs = generate_sub_questions(topic)
    print("\nGenerated Sub-Questions:")
    for q in sub_qs:
        print("-", q)

    # Step 2
    answers = []
    print("\nAnswers from Tavily Search:")
    for q in sub_qs:
        ans = search_answer(q)
        print(f"\nQ: {q}\nA: {ans}")
        answers.append(ans)

    # Step 3
    final_summary = summarize(topic, answers)
    print("\n=== FINAL SUMMARY ===\n", final_summary)


if __name__ == "__main__":
    main()

pip install langgraph langchain langchain-openai tavily-python

from tavily import TavilyClient
from openai import OpenAI

client = OpenAI(api_key="YOUR_OPENAI_API_KEY")
tavily = TavilyClient(api_key="YOUR_TAVILY_KEY")

topic = input("Topic: ")

# Generate sub questions
sub = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": f"Generate 3 sub-questions on {topic}"}]
).choices[0].message.content.split("\n")

cleaned = [q[q.find('.')+1:].strip() for q in sub]

answers = []
for q in cleaned:
    data = tavily.search(query=q, max_results=3)
    text = "\n".join([item["content"] for item in data["results"]][:3])
    ans = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"Summarize:\n{text}"}]
    ).choices[0].message.content
    answers.append(ans)

# Final Summary
final = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "\n".join(answers)}]
).choices[0].message.content

print("\nSummary:\n", final)

topic = input("Enter your research topic: ")

Enter your research topic: Climate change impact on agriculture

from openai import OpenAI

client = OpenAI(api_key="YOUR_API_KEY")

# ---------------------------------------
# STEP 1 — Generate 3 sub-questions
# ---------------------------------------
def generate_sub_questions(topic):
    prompt = f"""
    Generate exactly 3 meaningful sub-questions about:
    "{topic}"

    Return only the questions as:
    1. ...
    2. ...
    3. ...
    """

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=150
    )

    text = response.choices[0].message.content.strip().split("\n")
    cleaned = [q[q.find('.')+1:].strip() for q in text]
    return cleaned


# ---------------------------------------
# STEP 2 — Get short answers
# ---------------------------------------
def get_short_answer(question):
    prompt = f"Answer in 3–4 lines: {question}"

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=120
    )

    return response.choices[0].message.content.strip()


# ---------------------------------------
# STEP 3 — Final summary
# ---------------------------------------
def summarize(topic, answers):
    combined = "\n".join(answers)

    prompt = f"""
    Topic: {topic}
    Summarize the following points into one clear paragraph:

    {combined}
    """

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=150
    )

    return response.choices[0].message.content.strip()


# ---------------------------------------
# RUN FULL PIPELINE
# ---------------------------------------
topic = input("Enter your research topic: ")

print("\n=== Research Topic ===")
print(topic)

# Step 1
sub_qs = generate_sub_questions(topic)
print("\n--- Generated Sub-Questions ---")
for i, q in enumerate(sub_qs, 1):
    print(f"{i}. {q}")

# Step 2
answers = []
print("\n--- Short Answers ---")
for q in sub_qs:
    ans = get_short_answer(q)
    print(f"\nQ: {q}\nA: {ans}\n")
    answers.append(ans)

# Step 3
summary = summarize(topic, answers)
print("\n=== FINAL SUMMARY ===\n")
print(summary)

from langgraph.graph import StateGraph, END
from pydantic import BaseModel
from typing import List, Dict
from langchain_openai import ChatOpenAI
from tavily import TavilyClient
import os

# API Keys
os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_KEY"
os.environ["TAVILY_API_KEY"] = "YOUR_TAVILY_KEY"

llm = ChatOpenAI(model="gpt-4o-mini")
tavily = TavilyClient()

# Shared State across agents
class ResearchState(BaseModel):
    topic: str
    tasks: List[str] = []
    search_results: Dict[str, str] = {}
    final_summary: str = ""

def planner_agent(state: ResearchState):
    prompt = f"""
    You are a Planner Agent.

    Break the research topic into EXACTLY 3 clear, actionable sub-questions.
    Return them as a list only.

    Topic: {state.topic}
    """

    response = llm.invoke(prompt)
    lines = response.content.split("\n")
    tasks = [l[l.find('.')+1:].strip() for l in lines if '.' in l]

    state.tasks = tasks
    return state

def searcher_agent(state: ResearchState):
    results = {}

    for task in state.tasks:
        query = task

        # Tavily Web Search
        data = tavily.search(query=query, max_results=4)

        # Extract text from Tavily output
        text = "\n".join([item["content"] for item in data["results"]])

        # Summarize using LLM
        summary = llm.invoke(
            f"Summarize the following information in 5-6 lines:\n{text}"
        ).content

        results[task] = summary

    state.search_results = results
    return state

def writer_agent(state: ResearchState):
    combined_text = ""

    for task, info in state.search_results.items():
        combined_text += f"\n### {task}\n{info}\n"

    prompt = f"""
    You are a Writer Agent.

    Use the information below to create ONE structured, easy-to-read
    summary on the topic: "{state.topic}".

    Include:
    - Introduction
    - Key insights from each task
    - Final conclusion

    Information:
    {combined_text}
    """

    final_summary = llm.invoke(prompt).content
    state.final_summary = final_summary

    return state

workflow = StateGraph(ResearchState)

workflow.add_node("PLANNER", planner_agent)
workflow.add_node("SEARCHER", searcher_agent)
workflow.add_node("WRITER", writer_agent)

workflow.set_entry_point("PLANNER")

workflow.add_edge("PLANNER", "SEARCHER")
workflow.add_edge("SEARCHER", "WRITER")
workflow.add_edge("WRITER", END)

app = workflow.compile()

def run_research():
    topic = input("Enter your research topic: ")

    result = app.invoke({"topic": topic})

    print("\n===== SUB-TASKS GENERATED BY PLANNER =====")
    for t in result["tasks"]:
        print("-", t)

    print("\n===== SEARCH RESULTS FROM TAVILY =====")
    for k, v in result["search_results"].items():
        print(f"\n### {k}\n{v}")

    print("\n===== FINAL SUMMARY BY WRITER AGENT =====")
    print(result["final_summary"])


# Run the system
run_research()

pip install openai tavily-python langgraph pydantic tenacity

export OPENAI_API_KEY="sk-...."
export TAVILY_API_KEY="tavily-...."

$env:OPENAI_API_KEY="sk-...."
$env:TAVILY_API_KEY="tavily-...."

"""
Robust LangGraph multi-agent research pipeline:
Planner -> Searcher (Tavily) -> Writer

Make sure OPENAI_API_KEY and TAVILY_API_KEY are set in env.
"""

import os
import time
import json
from typing import List, Dict
from pydantic import BaseModel
from langgraph.graph import StateGraph, END
import openai
from tavily import TavilyClient
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# ---------------------------
# Basic configuration & checks
# ---------------------------
OPENAI_KEY = os.environ.get("OPENAI_API_KEY")
TAVILY_KEY = os.environ.get("TAVILY_API_KEY")

if not OPENAI_KEY:
    raise RuntimeError("Missing OPENAI_API_KEY environment variable. Set it before running.")
if not TAVILY_KEY:
    raise RuntimeError("Missing TAVILY_API_KEY environment variable. Set it before running.")

openai.api_key = OPENAI_KEY
tavily = TavilyClient(api_key=TAVILY_KEY)

# Helpful model config (change if needed)
LLM_MODEL = "gpt-4o-mini"  # or "gpt-4o" etc. Adjust if you have model access.
REQUEST_TIMEOUT = 20  # seconds


# ---------------------------
# Helper utilities
# ---------------------------
def debug_print(title: str, obj):
    print(f"\n--- {title} ---")
    if isinstance(obj, (dict, list)):
        print(json.dumps(obj, indent=2, ensure_ascii=False)[:4000])
    else:
        print(str(obj)[:4000])


@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=6),
       retry=retry_if_exception_type(Exception))
def call_openai_chat(messages: List[dict], max_tokens: int = 300):
    """Call OpenAI ChatCompletion with retries and helpful exception messages."""
    try:
        resp = openai.ChatCompletion.create(
            model=LLM_MODEL,
            messages=messages,
            max_tokens=max_tokens,
            temperature=0.2,
            request_timeout=REQUEST_TIMEOUT
        )
        return resp
    except Exception as e:
        # Bubble a clearer error for the retry wrapper
        print(f"[openai] transient/error: {e}. Retrying...")
        raise


def parse_numbered_list(text: str) -> List[str]:
    """Turn a numbered list into clean list of strings."""
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    items = []
    for ln in lines:
        # support "1. Question", "1) Question", "- Question", or raw lines
        if ln.lstrip().startswith(("1.", "2.", "3.", "1)", "2)", "3)")):
            pos = ln.find('.') if '.' in ln else ln.find(')')
            items.append(ln[pos+1:].strip())
        elif ln.startswith(("-", "*")):
            items.append(ln[1:].strip())
        else:
            items.append(ln)
    return items


# ---------------------------
# LangGraph State model
# ---------------------------
class ResearchState(BaseModel):
    topic: str
    tasks: List[str] = []
    search_results: Dict[str, str] = {}
    final_summary: str = ""


# ---------------------------
# Planner Agent
# ---------------------------
def planner_agent(state: ResearchState) -> ResearchState:
    if not state.topic or not state.topic.strip():
        raise ValueError("Planner Agent received empty topic.")

    prompt = (
        "You are a Planner Agent. Break the following research topic into EXACTLY 3 clear, "
        "actionable sub-questions/tasks. Return only the list in numbered form (1., 2., 3.).\n\n"
        f"Topic: {state.topic}\n\n"
        "Example desired output:\n1. First sub-question...\n2. Second sub-question...\n3. Third sub-question..."
    )

    messages = [{"role": "system", "content": "You are an expert research planner."},
                {"role": "user", "content": prompt}]

    resp = call_openai_chat(messages, max_tokens=200)
    text = resp["choices"][0]["message"]["content"].strip()
    tasks = parse_numbered_list(text)

    if len(tasks) < 3:
        # Fallback: attempt simple split by sentences (best-effort)
        print("[planner_agent] Warning: LLM returned fewer than 3 items. Using fallback splitting.")
        fallback = [t.strip() for t in text.split('.') if t.strip()]
        tasks = (fallback + [""] * 3)[:3]  # make sure 3 items

    state.tasks = tasks[:3]
    debug_print("Planner output", state.tasks)
    return state


# ---------------------------
# Searcher Agent
# ---------------------------
def searcher_agent(state: ResearchState) -> ResearchState:
    if not state.tasks:
        raise ValueError("Searcher Agent received empty tasks list.")

    results = {}
    for task in state.tasks:
        query = task
        print(f"\n[searcher_agent] Searching for: {query}")

        try:
            # Tavily search with a small timeout; wrap in try/except
            tavily_response = tavily.search(query=query, max_results=4)
            # tavily_response expected structure: {"results":[{"content": "...", "url": "...", ...}, ...]}
            hits = tavily_response.get("results", [])
            if not hits:
                print(f"[searcher_agent] No results for query: {query}.")
                summary_text = "No online sources found for this sub-question."
            else:
                # concatenate top snippets (best-effort)
                snippets = []
                for h in hits[:4]:
                    content = h.get("content") or h.get("snippet") or ""
                    url = h.get("url") or ""
                    snippet_line = content.strip()
                    if url:
                        snippet_line += f"\n(Source: {url})"
                    snippets.append(snippet_line)
                combined = "\n\n".join(snippets)
                # Summarize the combined content with the LLM
                summar_prompt = (
                    "You are a helpful summarizer. Summarize the following retrieved content "
                    "into a concise 4-6 line summary focusing on facts and citations where available.\n\n"
                    f"Content:\n{combined}\n\nSummary:"
                )
                messages = [{"role": "system", "content": "You are an expert summarizer."},
                            {"role": "user", "content": summar_prompt}]
                resp = call_openai_chat(messages, max_tokens=260)
                summary_text = resp["choices"][0]["message"]["content"].strip()
        except Exception as e:
            # Continue but include the error info for this task
            summary_text = f"[Error fetching/summarizing for this task: {e}]"
            print(f"[searcher_agent] Error for task '{task}': {e}")

        results[task] = summary_text
        debug_print(f"Searcher result for: {task}", summary_text)
        # polite pause to avoid rate limits
        time.sleep(0.5)

    state.search_results = results
    return state


# ---------------------------
# Writer Agent
# ---------------------------
def writer_agent(state: ResearchState) -> ResearchState:
    if not state.search_results:
        raise ValueError("Writer Agent received no search results.")

    combined_text = ""
    for task, summary in state.search_results.items():
        combined_text += f"\n### {task}\n{summary}\n"

    prompt = (
        "You are a Writer Agent. Using the information below, produce ONE cohesive, readable, "
        "structured research summary with: (1) brief introduction, (2) bullet/key insights for each sub-question, "
        "and (3) a short conclusion. Keep it professional and cite sources inline when available.\n\n"
        f"Information:{combined_text}\n\nFinal summary:"
    )

    messages = [{"role": "system", "content": "You are an expert technical writer."},
                {"role": "user", "content": prompt}]

    resp = call_openai_chat(messages, max_tokens=400)
    final = resp["choices"][0]["message"]["content"].strip()
    state.final_summary = final
    debug_print("Writer final summary", final)
    return state


# ---------------------------
# Build LangGraph workflow
# ---------------------------
workflow = StateGraph(ResearchState)
workflow.add_node("PLANNER", planner_agent)
workflow.add_node("SEARCHER", searcher_agent)
workflow.add_node("WRITER", writer_agent)

workflow.set_entry_point("PLANNER")
workflow.add_edge("PLANNER", "SEARCHER")
workflow.add_edge("SEARCHER", "WRITER")
workflow.add_edge("WRITER", END)

app = workflow.compile()


# ---------------------------
# Runner
# ---------------------------
def run_research():
    topic = input("Enter your research topic: ").strip()
    if not topic:
        print("Error: topic cannot be empty. Please provide a topic and try again.")
        return

    try:
        result = app.invoke({"topic": topic})
    except Exception as e:
        print(f"\n[ERROR] Pipeline failed: {e}")
        print("Common causes: invalid API keys, network error, or model access restricted.")
        raise

    print("\n\n=== Planner: Sub-tasks ===")
    for t in result.get("tasks", []):
        print("-", t)

    print("\n\n=== Search Results (summaries) ===")
    for k, v in result.get("search_results", {}).items():
        print(f"\n--- {k} ---\n{v}\n")

    print("\n\n=== Final Summary ===")
    print(result.get("final_summary", ""))


if __name__ == "__main__":
    run_research()

import openai, os
openai.api_key = os.environ.get("OPENAI_API_KEY")
print(openai.ChatCompletion.create(model="gpt-4o-mini", messages=[{"role":"user","content":"hello"}]))

from tavily import TavilyClient
client = TavilyClient(api_key=os.environ.get("TAVILY_API_KEY"))
print(client.search(query="what is langchain", max_results=1))